# -*- coding: utf-8 -*-
"""AI_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14Q5jSXB1YXtA6pYoHHOPh9I9d4-4Pivk
"""

import pandas as pd
import openpyxl
data = pd.read_excel("/content/drive/MyDrive/AI data.xlsx")
# Display the column names
print("Column Names:")
print(data.columns)
print(data.head(29))

!pip install pytesseract
!pip install opencv-python

!sudo apt-get install tesseract-ocr

import pandas as pd
import pytesseract
from PIL import Image
import cv2
import numpy as np
import os

pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'


def extract_text_from_image(image_path):
    image = cv2.imread(image_path)
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    _, binary_image = cv2.threshold(gray_image, 128, 255, cv2.THRESH_BINARY_INV)

    # Perform OCR on the binary image
    text = pytesseract.image_to_string(Image.fromarray(binary_image))

    return text

data['Extracted_Text'] = data['SCREENSHOT'].apply(extract_text_from_image)

# Display the extracted text
print("Extracted Text from Screenshots:")
print(data[['GAME NAME', 'Extracted_Text']])

!pip install torchvision

import torch
from torchvision import transforms
from transformers import ViTFeatureExtractor, ViTForImageClassification, GPT2Tokenizer, GPT2Model
from torch.utils.data import Dataset, DataLoader
import pandas as pd

class GameDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # Assume 'SCREENSHOT' column contains the image paths and 'TEXT' column contains text data
        image_path = self.data.loc[idx, 'SCREENSHOT']
        text_data = self.data.loc[idx, 'TEXT']
        image_tensor = load_and_preprocess_image(image_path)
        text_tensor = tokenize_text(text_data)

        # Return a dictionary with 'image' and 'text' tensors
        return {'image': image_tensor, 'text': text_tensor}

def load_and_preprocess_image(image_path):
    image = cv2.imread(image_path)

    # Preprocess the image (adjust based on your needs)
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    preprocessed_image = transform(image)
    preprocessed_image = preprocessed_image.unsqueeze(0)  # Add batch dimension

    return preprocessed_image

# Assuming the "screenshot" column contains file paths to images
image_paths = data['SCREENSHOT']

# Load and preprocess each image in the dataset
for image_path in image_paths:
    image_tensor = load_and_preprocess_image(image_path)
    print(image_tensor.shape)

import torch
from transformers import ViTFeatureExtractor, ViTForImageClassification, GPT2Tokenizer, GPT2Model
from torch import nn

# Initialize ViT model and feature extractor
vit_model_name = 'google/vit-base-patch16-224-in21k'
feature_extractor = ViTFeatureExtractor.from_pretrained(vit_model_name)
vit_model = ViTForImageClassification.from_pretrained(vit_model_name)

# Initialize GPT model and tokenizer
gpt_model_name = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(gpt_model_name)
gpt_model = GPT2Model.from_pretrained(gpt_model_name)

class MultimodalModel(nn.Module):
    def __init__(self, vit_model, gpt_model):
        super(MultimodalModel, self).__init__()
        self.vit_model = vit_model
        self.gpt_model = gpt_model
        self.fc = nn.Linear(vit_model.config.hidden_size + gpt_model.config.hidden_size, num_classes)

    def forward(self, images, texts):

        image_features = self.vit_model(**feature_extractor(images))['last_hidden_state']

        text_features = self.gpt_model(**tokenizer(texts))['last_hidden_state']

        combined_features = torch.cat((image_features, text_features), dim=1)

        output = self.fc(combined_features)

        return output

# Instantiate the multimodal model
num_classes = 10
multimodal_model = MultimodalModel(vit_model, gpt_model)

!pip install DataLoader

import torch
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader
import torch.nn as nn
import torch.nn.functional as F
from torchvision import transforms

data = pd.read_excel("/content/drive/MyDrive/AI data.xlsx")

batch_size = 8
data_loader = DataLoader(data, batch_size=batch_size, shuffle=True)
print(len(data))


class MultimodalModel(nn.Module):
    def __init__(self, image_feature_dim, text_feature_dim, num_classes):
        super(MultimodalModel, self).__init__()

        # Define architecture for image branch
        self.fc_image = nn.Linear(512, image_feature_dim)
        self.dropout_image = nn.Dropout(0.3)

        # Define architecture for text branch (assuming text features are already extracted)
        self.fc_text = nn.Linear(text_feature_dim, 512)
        self.dropout_text = nn.Dropout(0.3)

        # Define a fusion layer to combine image and text features
        self.fc_fusion = nn.Linear(1024, 512)

        # Define the final output layer
        self.fc_output = nn.Linear(512, num_classes)

    def forward(self, image_features, text_features):

        image_features = image_features.view(-1, 512)
        text_features = text_features.view(-1, 512)

        # Forward pass through image branch
        image_features = F.relu(self.fc_image(image_features))
        image_features = self.dropout_image(image_features)

        # Forward pass through text branch
        text_features = F.relu(self.fc_text(text_features))
        text_features = self.dropout_text(text_features)

        # Concatenate image and text features
        combined_features = torch.cat((image_features, text_features), dim=1)

        # Fusion layer to combine multimodal features
        fused_features = F.relu(self.fc_fusion(combined_features))

        # Final output layer
        output = self.fc_output(fused_features)

        return output

    def load_and_preprocess_image(image_path):
    # Load image using PIL
        image = Image.open(image_path)

    # Preprocess the image (adjust based on your needs)
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
      ])

        tensor_image = image_to_tensor(image)
        return tensor_image

# Example usage:
image_path = "/content/drive/MyDrive/screenshots/Balloon_mater.jpeg"
tensor_image = load_and_preprocess_image(image_path)
print(tensor_image)

# Create an instance of your MultimodalModel
image_dim = 2048
text_dim = 768
num_classes = 29
multimodal_model = MultimodalModel(image_dim, text_dim, num_classes)

optimizer = optim.SGD(multimodal_model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    multimodal_model.train()
    total_loss = 0.0

    for i in range(0, len(data), batch_size):
         batch = data.iloc[i:i + batch_size]
         images = [load_and_preprocess_image(image_path) for image_path in batch['SCREENSHOT']]
         texts = batch['OBJECTIVE OF THE GAME'] + ' ' + batch['RULES OF THE GAME']

         # text processing logic goes here
         texts = [torch.tensor([ord(char) for char in text.lower()]) for text in texts]

         texts = pad_sequence(texts, batch_first=True, padding_value=0)

         labels = batch['GAME NAME']

         images = torch.stack(images)

         # Zero gradients, forward pass, compute loss, backward pass, optimize
         optimizer.zero_grad()
         outputs = multimodal_model(images, texts)
         loss = criterion(outputs, labels)
         loss.backward()
         optimizer.step()


         total_loss += loss.item()

    # Calculate average loss for the epoch
    avg_loss = total_loss / len(data_loader)

    # Print or log training metrics
    print(f"Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss}")

# Saved the trained model
torch.save(multimodal_model.state_dict(), 'multimodal_model.pth')